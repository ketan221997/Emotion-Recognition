# Importing the libraries
import cv2
import numpy as np
from keras.models import load_model
from statistics import mode
from utils.datasets import get_labels
from utils.inference import detect_faces
from utils.inference import draw_text
from utils.inference import draw_bounding_box
from utils.inference import apply_offsets
from utils.inference import load_detection_model
from utils.preprocessor import preprocess_input
import warnings

warnings.filterwarnings("ignore")

# parameters for loading data and images
emotion_model_path = './models/emotion_model.hdf5'
emotion_labels = get_labels('fer2013')

frame_window = 10#confirm with a minimum number of frames
emotion_offsets = (20, 40)

#_____________________________________YETI IS GOD________________________________________

# load our serialized model from disk
print("[INFO] loading model...")
net = cv2.dnn.readNetFromCaffe("deploy.prototxt.txt", "res10_300x300_ssd_iter_140000.caffemodel")

# load the input image and construct an input blob for the image
# by resizing to a fixed 300x300 pixels and then normalizing it



#_______________________________________GOD NEEDS SLEEP_______________________________________



face_cascade = cv2.CascadeClassifier('./models/haarcascade_frontalface_default.xml')
emotion_classifier = load_model(emotion_model_path)

# getting input model shapes for inference
emotion_target_size = emotion_classifier.input_shape[1:3]
    
# starting lists for calculating modes
emotion_window = []


cv2.namedWindow('window_frame')
video_capture = cv2.VideoCapture(0)

cap = cv2.VideoCapture(0)

while cap.isOpened(): # True:
    ret, bgr_image = cap.read()

    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)
    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)

    #faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,
#			minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)

    (h, w) = rgb_image.shape[:2]
    blob = cv2.dnn.blobFromImage(cv2.resize(rgb_image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))

    #for face_coordinates in faces: #Yeti


        

        # pass the blob through the network and obtain the detections and
        # predictions
    print("[INFO] computing object detections...")
    net.setInput(blob)
    detections = net.forward()

    # loop over the detections
    for i in range(0, detections.shape[2]):
        confidence = detections[0, 0, i, 2]

        if confidence > 0.5:
            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            (x1, y1, x2, y2) = box.astype("int")

            #text = "{:.2f}%".format(confidence * 100)
            y = y1 - 10 if y1 - 10 > 10 else y1 + 10        

            #x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)
            gray_face = gray_image[y1:y2, x1:x2]
            try:
                gray_face = cv2.resize(gray_face, (emotion_target_size))
            except:
                continue

            gray_face = preprocess_input(gray_face, True)
            gray_face = np.expand_dims(gray_face, 0)
            gray_face = np.expand_dims(gray_face, -1)
            emotion_prediction = emotion_classifier.predict(gray_face)
            emotion_probability = np.max(emotion_prediction)
            emotion_label_arg = np.argmax(emotion_prediction)
            emotion_text = emotion_labels[emotion_label_arg]
            emotion_window.append(emotion_text)

            if len(emotion_window) > frame_window:
                emotion_window.pop(0)
            try:
                emotion_mode = mode(emotion_window)
            except:
                continue

            if emotion_text != 'happy':
                emotion_text = 'neutral'

            #based on emotion, decide the color
            if emotion_text == 'angry':
                color = emotion_probability * np.asarray((255, 0, 0))
            elif emotion_text == 'sad':
                color = emotion_probability * np.asarray((0, 0, 255))
            elif emotion_text == 'happy':
                color = emotion_probability * np.asarray((255, 255, 0))
            elif emotion_text == 'surprise':
                color = emotion_probability * np.asarray((0, 255, 255))
            else:
                color = emotion_probability * np.asarray((0, 255, 0))

            color = color.astype(int)
            color = color.tolist()

            x_offset=y_offset = 100
            
            s_img = cv2.imread(["smiling.png", "neutrall.png"][emotion_text != 'happy'], -1)
            s_img = cv2.resize(s_img, (100, 100))
            
            #cv2.imshow("emoji", s_img)
            

##            y11, y22 = y_offset, y_offset + s_img.shape[0]
##            x11, x22 = x_offset, x_offset + s_img.shape[1]
            sz = 3
##            if emotion_text == 'neutral':
##                sz -= 1
            alpha_s = s_img[:, :, sz] / 255.0
            alpha_l = 1.0 - alpha_s

            cv2.resize(rgb_image, (360, 480))
##            bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)#Yeti

            y11 = max(0, y1)
            y22 = y11 + s_img.shape[0]
            x11 = max(0, x1- 100)
            x22 = x11 + s_img.shape[1]
            
            s_img = cv2.cvtColor(s_img, cv2.COLOR_BGR2RGB)
            
            for c in range(3):
                rgb_image[y11:y22, x11:x22, c] = (alpha_s * s_img[:, :, c] +
                                          alpha_l * rgb_image[y11:y22, x11:x22, c])

            #cv2.imshow("frame", cv2.resize(l_img, (520, 480)))


            #drawing a bounding box
            
            draw_bounding_box([x1, y1, x2 - x1, y2 - y1], rgb_image, color)
##            draw_text([x1, y1, x2 - x1, y2 - y1], bgr_image, emotion_mode,  # Yeti removed it
##                      color, 0, -45, 1, 1)

        bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)
        cv2.imshow('window_frame', cv2.resize(bgr_image, (520, 480)))
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()
